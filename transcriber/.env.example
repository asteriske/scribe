# Transcriber Service Configuration
# Copy this file to .env and update with your settings

# ===== Service Configuration =====
HOST=0.0.0.0
PORT=8001
WORKERS=1
LOG_LEVEL=INFO

# ===== Model Configuration =====
# Options: tiny, base, small, medium, large-v3
WHISPER_MODEL=medium

# Where to cache model files
MODEL_DIR=~/.cache/whisper

# ===== Job Queue Configuration =====
# Maximum number of concurrent transcription jobs (recommend 1 for MLX)
MAX_CONCURRENT_JOBS=1

# Maximum number of jobs in queue
QUEUE_SIZE=10

# How long to keep completed job results in memory (hours)
JOB_RETENTION_HOURS=1

# ===== Performance Configuration =====
# Compute type: float16 (faster, recommended) or float32
COMPUTE_TYPE=float16

# ===== Hallucination Mitigation =====
# Prevent repetition loops by not conditioning on previous text
CONDITION_ON_PREVIOUS_TEXT=false

# Compression ratio above this triggers temperature fallback (detects repetition)
COMPRESSION_RATIO_THRESHOLD=2.4

# Segments with no-speech probability above this are suppressed
NO_SPEECH_THRESHOLD=0.6

# Segments with avg log probability below this trigger temperature fallback
LOGPROB_THRESHOLD=-1.0

# Comma-separated temperatures tried in order on failed segments
TEMPERATURE=0.0,0.2,0.4,0.6,0.8,1.0

# Skip hallucinated text during silent periods longer than this (seconds)
# Set to empty to disable (requires word_timestamps, enabled automatically)
HALLUCINATION_SILENCE_THRESHOLD=2.0

# Optional prompt to prime the model (e.g. proper nouns, domain terms)
# INITIAL_PROMPT=

# ===== Logging Configuration =====
# Log file path (relative to project root)
LOG_FILE=data/logs/transcriber.log

# Log format: json or text
LOG_FORMAT=text
